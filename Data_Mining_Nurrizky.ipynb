{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Mining_Nurrizky.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgAm1IsgCj4KS2rFV/Zy8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurrizkyimani/mesh_on_demand/blob/master/Data_Mining_Nurrizky.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "id": "Mel2OEmJ5jCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31450fa-9984-431c-fbe7-a5e23d19711e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import "
      ],
      "metadata": {
        "id": "0SNomm5v51or"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApX6qtqwMAyW"
      },
      "outputs": [],
      "source": [
        "# !pip install spacy==2.3.0\n",
        "# import spacy\n",
        "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_ner_bionlp13cg_md-0.2.5.tar.gz\n",
        "# import en_ner_bionlp13cg_md\n",
        "!pip install spacy==2.3.5\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
        "import re, nltk, spacy, gensim\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import string\n",
        "import itertools\n",
        "import io\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "GD0tIJ-D53NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = ToktokTokenizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "punct = punctuation\n",
        "\n",
        "def clean_text(text):\n",
        "    ''' Lowering text and removing undesirable marks\n",
        "    '''\n",
        "    \n",
        "    text = re.sub(\"\\d+\", \"\", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
        "    text = re.sub('\\s+', ' ', text) # matches all whitespace characters\n",
        "    text = text.strip(' ')\n",
        "    return text\n",
        "  \n",
        "def strip_list_noempty(mylist):\n",
        "    \n",
        "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
        "    return [item for item in newlist if item != '']\n",
        "    \n",
        "    \n",
        "def clean_punct(text): \n",
        "    ''' Remove punctuations'''\n",
        "    words = token.tokenize(text)\n",
        "    punctuation_filtered = []\n",
        "    regex = re.compile('[%s]' % re.escape(punct))\n",
        "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
        "    \n",
        "    for w in words:\n",
        "        w = re.sub('^[0-9]*', \" \", w)\n",
        "        punctuation_filtered.append(regex.sub('', w))\n",
        "  \n",
        "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
        "        \n",
        "    return ' '.join(map(str, filtered_list))\n",
        "  \n",
        "def stopWordsRemove(text):\n",
        "    ''' Removing all the english stop words from a corpus\n",
        "    Parameter:\n",
        "    text: corpus to remove stop words from it\n",
        "    '''\n",
        "    words = token.tokenize(text)\n",
        "    filtered = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    return ' '.join(map(str, filtered))\n",
        "  \n",
        "def lemmatization(texts, allowed_postags, stop_words=stop_words):\n",
        "    ''' It keeps the lemma of the words (lemma is the uninflected form of a word),\n",
        "    and deletes the underired POS tags\n",
        "    \n",
        "    Parameters:\n",
        "    \n",
        "    texts (list): text to lemmatize\n",
        "    allowed_postags (list): list of allowed postags, like NOUN, ADL, VERB, ADV\n",
        "    '''\n",
        "    lemma = wordnet.WordNetLemmatizer()       \n",
        "    doc = nlp(texts) \n",
        "    texts_out = []\n",
        "    \n",
        "    for token in doc:\n",
        "        if token.pos_ in allowed_postags:\n",
        "            \n",
        "          if token.lemma_ not in ['-PRON-']:\n",
        "              texts_out.append(token.lemma_)\n",
        "              \n",
        "          else:\n",
        "              texts_out.append('')\n",
        "     \n",
        "    texts_out = ' '.join(texts_out)\n",
        "\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "Zkd7wasQ521d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "1PnakyDWF0E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tf5b6Qh1F4bk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}